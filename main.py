import os
import base64
import datetime
import random
import re
import requests
import gspread
from google.oauth2.service_account import Credentials
import html
from bs4 import BeautifulSoup
import time

# ----------------------------------------------------------------------
# 0. åŸºæœ¬è¨­å®š
# ----------------------------------------------------------------------
print(f"â–¶ TEST_MODE: {os.getenv('TEST_MODE')}")

SPREADSHEET_ID  = os.environ['SPREADSHEET_ID']
CLAUDE_API_KEY  = os.environ['CLAUDE_API_KEY']
SCOPES          = ["https://www.googleapis.com/auth/spreadsheets"]

MAX_PAGES        = 3     # ã‚¹ãƒ¬ä¸€è¦§ã‚’å·¡å›ã™ã‚‹æœ€å¤§ãƒšãƒ¼ã‚¸
POST_COUNT       = 14    # æŠ•ç¨¿å€™è£œã¨ã—ã¦æ¡ç”¨ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰æ•°
MAX_RETRY_BASE   = 3     # generate_summary() å†…éƒ¨ãƒªãƒˆãƒ©ã‚¤å›æ•°
MAX_EXTRA_RETRY  = 2     # "NOK"/ç¦æ­¢èªã®è¿½åŠ ãƒªãƒˆãƒ©ã‚¤å›æ•°

HISTORY_SHEET    = "ã‚¹ãƒ¬å±¥æ­´"
CANDIDATE_SHEET  = "æŠ•ç¨¿å€™è£œ"
POST_SHEET       = "æŠ•ç¨¿äºˆå®š"

# ----------------------------------------------------------------------
# 1. Google èªè¨¼
# ----------------------------------------------------------------------
print("â–¶ Decoding GCP_SERVICE_ACCOUNT_B64...")
json_bytes = base64.b64decode(os.environ['GCP_SERVICE_ACCOUNT_B64'])
with open("service_account.json", "wb") as f:
    f.write(json_bytes)
print("â–¶ service_account.json written")

print("â–¶ Authorizing gspread...")
CREDS = Credentials.from_service_account_file("service_account.json", scopes=SCOPES)
GC    = gspread.authorize(CREDS)
print("â–¶ gspread authorized")

# ----------------------------------------------------------------------
# 2. ç¦æ­¢èªãƒªã‚¹ãƒˆ
# ----------------------------------------------------------------------
BANNED_WORDS = [
    "æ„å‘³ä¸æ˜", "å…±ç”£ä¸»ç¾©", "ä¸­å›½äºº", "è¡€ç¨", "ç³å°¿",
    "æ‚©ã‚€", "ã‚¹ã‚±ãƒ™", "ä½ä¿—", "ãƒˆãƒ©ãƒ–ãƒ«", "é…·ã„", "åŠ£ç­‰æ„Ÿ", "ä¸‰æµ", "ã‚¿ã‚¤ãƒˆãƒ«"
]

def contains_banned(words: list[str], text: str) -> bool:
    return any(re.search(re.escape(w), text, re.IGNORECASE) for w in words)

# ----------------------------------------------------------------------
# 3. ã‚¹ãƒ¬ãƒƒãƒ‰å–å¾—é–¢é€£
# ----------------------------------------------------------------------
def fetch_threads():
    threads = []
    ua = {"User-Agent": "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"}
    for p in range(1, MAX_PAGES + 1):
        url = f"https://www.e-mansion.co.jp/bbs/board/23ku/?page={p}"
        print(f"â–¶ Fetching list page: {url}")
        res = requests.get(url, headers=ua, timeout=30)
        if res.status_code != 200:
            print(f"â–¶ ERROR: status {res.status_code}")
            continue
        blocks = re.findall(
            r'<a href="/bbs/thread/(\d+)/" class="component_thread_list_item link.*?<span class="num_of_item">(\d+)</span>.*?<div class="oneliner title"[^>]*>(.*?)</div>',
            res.text, re.DOTALL
        )
        for tid, cnt, ttl in blocks:
            threads.append({
                "url": f"https://www.e-mansion.co.jp/bbs/thread/{tid}/",
                "title": html.unescape(ttl).strip(),
                "count": int(cnt),
                "id": tid
            })
        print(f"â–¶  Page {p}: found {len(blocks)} threads")
    return threads

def fetch_thread_posts(tid: str, max_pages: int = 5, delay: float = 1.0) -> list[str]:
    ua = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    posts = []
    for p in range(1, max_pages + 1):
        url = f"https://www.e-mansion.co.jp/bbs/thread/{tid}/?page={p}"
        try:
            r = requests.get(url, headers=ua, timeout=15)
            r.raise_for_status()
        except requests.RequestException as e:
            print(f"â–¶ Error: {e}")
            continue
        soup = BeautifulSoup(r.text, "html.parser")
        posts += [t.get_text(strip=True) for t in soup.select('p[itemprop="commentText"]') if t.get_text(strip=True)]
        time.sleep(delay)
    print(f"â–¶ Thread {tid}: collected {len(posts)} posts")
    return posts

def fetch_thread_text(url: str) -> str:
    tid = re.search(r'/thread/(\d+)/', url).group(1)
    return "\n".join(fetch_thread_posts(tid))

# ----------------------------------------------------------------------
# 4. ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆå±¥æ­´
# ----------------------------------------------------------------------
def load_history() -> dict[str, int]:
    sheet = GC.open_by_key(SPREADSHEET_ID).worksheet(HISTORY_SHEET)
    return {r[0]: int(r[1]) for r in sheet.get_all_values()[1:] if len(r) > 1 and r[1].isdigit()}

def save_history(hist: dict[str, int]):
    ws = GC.open_by_key(SPREADSHEET_ID).worksheet(HISTORY_SHEET)
    ws.clear()
    ws.append_row(["URL", "å–å¾—æ™‚ãƒ¬ã‚¹æ•°", "æœ€çµ‚å–å¾—æ—¥"])
    for url, cnt in hist.items():
        ws.append_row([url, cnt, datetime.datetime.now().strftime("%Y/%m/%d")])

# ----------------------------------------------------------------------
# 5. ç‚ä¸Šãƒªã‚¹ã‚¯ (2 æ®µéš)
# ----------------------------------------------------------------------
def judge_risk(text: str):
    prompt = f"""ä»¥ä¸‹ã®æ²ç¤ºæ¿æ›¸ãè¾¼ã¿ã®ç‚ä¸Šãƒªã‚¹ã‚¯ã‚’åˆ¤å®šã—ã€æœ€åˆã«ã€Œãƒªã‚¹ã‚¯ï¼šé«˜ã€ã¾ãŸã¯ã€Œãƒªã‚¹ã‚¯ï¼šä½ã€ã‚’æ˜ç¤ºã—ã¦æ ¹æ‹ ã‚’ç°¡æ½”ã«è¿°ã¹ã¦ãã ã•ã„ã€‚
--- æœ¬æ–‡ ---
{text}"""
    payload = {
        "model": "claude-3-haiku-20240307",
        "temperature": 0,
        "max_tokens": 300,
        "messages": [{"role": "user", "content": prompt}]
    }
    headers = {"x-api-key": CLAUDE_API_KEY, "anthropic-version": "2023-06-01"}
    try:
        res = requests.post("https://api.anthropic.com/v1/messages", headers=headers,
                            json=payload, timeout=45)
        msg = res.json()["content"][0]["text"].strip()
        risk = "é«˜" if "ãƒªã‚¹ã‚¯ï¼šé«˜" in msg else "ä½"
        flag = "NG" if risk == "é«˜" else "OK"
        return risk, msg, flag
    except Exception as e:
        return "é«˜", f"[Error] {e}", "NG"

# ----------------------------------------------------------------------
# 6. ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ (NOK / API ã‚¨ãƒ©ãƒ¼å¯¾ç­–)
# ----------------------------------------------------------------------
def generate_summary(text: str, max_retry: int = MAX_RETRY_BASE) -> str:
    base_prompt = f"""
ã‚ãªãŸã¯ Xï¼ˆæ—§Twitterï¼‰å‘ã‘ã®ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚  
æ²ç¤ºæ¿ã‚¹ãƒ¬ãƒƒãƒ‰æœ¬æ–‡ã‚’èª­ã¿ã€èª­è€…ãŒç¶šãã‚’ã‚¯ãƒªãƒƒã‚¯ã—ãŸããªã‚‹ **å‰å‘ãã§é•·ã‚** ã®æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ã‚’ 1 æœ¬ã ã‘ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
### å‡ºåŠ›ä»•æ§˜ï¼ˆå¿…ãšå®ˆã‚‹ï¼‰
1. **ã‚¿ã‚¤ãƒˆãƒ«æœ¬æ–‡ã®ã¿** ã‚’ 1 è¡Œã§å‡ºåŠ›
   - è¡Œé ­ã«ã€Œ-ã€ã€Œâ€“ã€ã€Œãƒ»ã€ãªã©è¨˜å·ã‚’ä»˜ã‘ãªã„
   - æ¥é ­è¾ã€Œã‚¿ã‚¤ãƒˆãƒ«:ã€ã‚„è§£èª¬æ–‡ï¼ˆä¾‹: ã€Œç¦æ­¢èªã‚’å«ã¾ãšï½ã€ã€Œ120æ–‡å­—ä»¥å†…ã§ï½ã€ï¼‰ã‚’ä»˜ã‘ãªã„
   - åŒä¸€ã®æ–‡ã‚’ 2 å›ä»¥ä¸Šç¹°ã‚Šè¿”ã•ãªã„
   - æ”¹è¡Œãƒ»ã‹ãæ‹¬å¼§ãƒ»ç®‡æ¡æ›¸ããƒ»çµµæ–‡å­—ãƒ»è¨˜å·èª¬æ˜ã‚’ä»˜ã‘ãªã„
   - 120 æ–‡å­—ä»¥å†…
2. æœ¬æ–‡å†’é ­ã« **åœ°åãƒ»é§…åãƒ»æ•°å­—** ã„ãšã‚Œã‹ã‚’å…¥ã‚Œã¦ç›®ã‚’å¼•ãæ§‹æˆã«ã™ã‚‹
3. æœ«å°¾ã« **4ï½6 æ–‡å­—ç¨‹åº¦ã® CTA** ã‚’ä»˜ã‘ã‚‹ï¼ˆä¾‹: è©³ã—ãã¯ã“ã¡ã‚‰ğŸ‘‡ã€ç¶šãã¯ã“ã¡ã‚‰â†’ï¼‰
4. ä¸Šè¨˜ã‚’ 1 ã¤ã§ã‚‚æº€ãŸã›ãªã„ã€ã¾ãŸã¯ç¦æ­¢èªã‚’ 1 èªã§ã‚‚å«ã‚€å ´åˆã¯ **NOK** ã¨ã ã‘å‡ºåŠ›ã™ã‚‹
   - NOK ä»¥å¤–ã®æ–‡å­—ã‚’ä¸€åˆ‡æ›¸ã‹ãªã„
### ç¦æ­¢èª
{', '.join(BANNED_WORDS)}
--- æœ¬æ–‡ ---
{text}"""
    headers = {"x-api-key": CLAUDE_API_KEY, "anthropic-version": "2023-06-01"}

    for i in range(max_retry + 1):
        body = {
            "model": "claude-3-haiku-20240307",
            "temperature": 0,
            "max_tokens": 100,
            "messages": [{"role": "user", "content": base_prompt}]
        }
        try:
            res = requests.post("https://api.anthropic.com/v1/messages",
                                headers=headers, json=body, timeout=40)
            if res.status_code != 200:
                print(f"â–¶ Claude HTTP {res.status_code}: retry {i}/{max_retry}")
                time.sleep(3)
                continue
            data = res.json()
            if "content" not in data:
                print(f"â–¶ Claude JSON without 'content': retry {i}/{max_retry}")
                time.sleep(3)
                continue
            title = data["content"][0]["text"].strip()
        except Exception as e:
            print(f"â–¶ Claude request error: {e}  retry {i}/{max_retry}")
            time.sleep(3)
            continue

        # NOK æ–‡ç« ã‚’å³ NOK æ‰±ã„
        if "NOK" in title.upper():
            return "NOK"

        title = re.sub(r'^\s*ã‚¿ã‚¤ãƒˆãƒ«[:ï¼š]\s*', '', title)
        if title.startswith(("ã€Œ", "\"", "ã€")) and title.endswith(("ã€", "\"", "ã€")):
            title = title[1:-1]
        title = re.sub(r'^.*?[ã€Œ"](.*?)[ã€"]$', r'\1', title)

        if not contains_banned(BANNED_WORDS, title):
            return title
        time.sleep(1)
    return "NOK"

# ----------------------------------------------------------------------
# 7. ãƒ¡ã‚¤ãƒ³
# ----------------------------------------------------------------------
def main():
    print("â–¶ main() start")
    threads   = fetch_threads()
    history   = load_history()
    seen_ids  = set()
    diffs     = []

    for t in sorted(threads, key=lambda x: x["count"] - history.get(x["url"], 0), reverse=True):
        if t["id"] in seen_ids:
            continue
        seen_ids.add(t["id"])

        prev = history.get(t["url"], 0)
        diff = t["count"] - prev
        if diff <= 0 and t["url"] in history:
            continue
        if t["url"] not in history and t["count"] < 100:
            continue
        diffs.append({**t, "diff": diff})
        if len(diffs) == 20:
            break

    candidates, updated = [], {}
    for d in diffs:
        text = fetch_thread_text(d["url"])
        if not text:
            continue
        risk, msg, flag = judge_risk(text)
        candidates.append({**d, "risk": risk, "comment": msg, "flag": flag})
        updated[d["url"]] = d["count"]

    ok = [c for c in candidates if c["flag"] == "OK"][:POST_COUNT]
    random.shuffle(ok)

    if os.getenv("TEST_MODE") != "1":
        new_hist = {**history, **{u: updated[u] for u in updated if u in [c["url"] for c in ok]}}
        save_history(new_hist)
    else:
        print("â–¶ TEST_MODE: history not saved")

    ws_cand = GC.open_by_key(SPREADSHEET_ID).worksheet(CANDIDATE_SHEET)
    ws_cand.clear()
    ws_cand.append_row(["URL", "å·®åˆ†ãƒ¬ã‚¹æ•°", "ã‚¿ã‚¤ãƒˆãƒ«", "ç‚ä¸Šãƒªã‚¹ã‚¯", "ã‚³ãƒ¡ãƒ³ãƒˆ", "æŠ•ç¨¿å¯å¦"])
    for c in sorted(candidates, key=lambda x: x["diff"], reverse=True):
        ws_cand.append_row([c["url"], c["diff"], c["title"], c["risk"], c["comment"], c["flag"]])

    ws_post = GC.open_by_key(SPREADSHEET_ID).worksheet(POST_SHEET)
    ws_post.clear()
    ws_post.append_row(["æ—¥ä»˜", "æŠ•ç¨¿æ™‚é–“", "æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆ", "æŠ•ç¨¿æ¸ˆã¿", "URL"])

    today = datetime.date.today()
    for idx, c in enumerate(ok):
        summary = "NOK"
        for n in range(MAX_EXTRA_RETRY + 1):
            summary = generate_summary(fetch_thread_text(c["url"]))
            if summary.upper() != "NOK" and not contains_banned(BANNED_WORDS, summary):
                break
            print(f"â–¶ Extra retry {n+1}/{MAX_EXTRA_RETRY} â†’ {c['title']}")
        if summary.upper() == "NOK" or contains_banned(BANNED_WORDS, summary):
            print(f"â–¶ Skip (still NG): {c['title']}")
            continue

        post_date = today + datetime.timedelta(days=1 + idx // 2)
        time_str  = "8:00" if idx % 2 == 0 else "15:00"
        tid       = re.search(r'/thread/(\d+)/', c["url"]).group(1)
        utm       = f"?utm_source=x&utm_medium=em-{tid}&utm_campaign={post_date.strftime('%Y%m%d')}"
        post_txt  = f"{summary}\n#ãƒãƒ³ã‚·ãƒ§ãƒ³ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£\n{c['url']}{utm}"
        ws_post.append_row([post_date.strftime("%Y/%m/%d"), time_str, post_txt, "FALSE", c["url"]])

    print("â–¶ Done")

# ----------------------------------------------------------------------
if __name__ == "__main__":
    main()
