#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒãƒ³ã‚·ãƒ§ãƒ³ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£è‡ªå‹•æŠ•ç¨¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
  - é€± 1 å›ï¼ˆé‡‘æ›œ 23:00JSTï¼‰ã®å®Ÿè¡Œã‚’æƒ³å®š
  - ç¿Œé€±æœˆæ›œã‹ã‚‰ 8:00 / 15:00 ã§æŠ•ç¨¿ã‚­ãƒ¥ãƒ¼ã‚’ç”Ÿæˆ
  - åŒã˜ã‚¹ãƒ¬ URL ã®é‡è¤‡ã‚’å®Œå…¨æ’é™¤
"""

import os, base64, datetime, random, re, time, requests, html
from bs4 import BeautifulSoup
import gspread
from google.oauth2.service_account import Credentials

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. ç’°å¢ƒè¨­å®š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"â–¶ TEST_MODE: {os.getenv('TEST_MODE')}")

SPREADSHEET_ID = os.environ["SPREADSHEET_ID"]
CLAUDE_API_KEY = os.environ["CLAUDE_API_KEY"]
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]

MAX_PAGES        = 3
POST_COUNT       = 14
MAX_RETRY_BASE   = 3     # Claude ãƒ™ãƒ¼ã‚¹ãƒªãƒˆãƒ©ã‚¤
MAX_EXTRA_RETRY  = 2     # ã‚¿ã‚¤ãƒˆãƒ« NG è¿½åŠ ãƒªãƒˆãƒ©ã‚¤

HISTORY_SHEET   = "ã‚¹ãƒ¬å±¥æ­´"
CANDIDATE_SHEET = "æŠ•ç¨¿å€™è£œ"
POST_SHEET      = "æŠ•ç¨¿äºˆå®š"

BANNED_WORDS = [
    "æ„å‘³ä¸æ˜", "å…±ç”£ä¸»ç¾©", "ä¸­å›½äºº", "è¡€ç¨", "ç³å°¿",
    "æ‚©ã‚€", "ã‚¹ã‚±ãƒ™", "ä½ä¿—", "ãƒˆãƒ©ãƒ–ãƒ«", "é…·ã„", "åŠ£ç­‰æ„Ÿ"
]

CTA = " è©³ã—ãã¯ã“ã¡ã‚‰ğŸ‘‡"
MAX_TITLE_LEN = 90 - len(CTA)  # â†’ 83 æ–‡å­—

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Google èªè¨¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
json_bytes = base64.b64decode(os.environ["GCP_SERVICE_ACCOUNT_B64"])
with open("service_account.json", "wb") as f:
    f.write(json_bytes)

creds = Credentials.from_service_account_file("service_account.json",
                                              scopes=SCOPES)
gc = gspread.authorize(creds)
print("â–¶ gspread authorized")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ãƒ˜ãƒ«ãƒ‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def contains_banned(words: list[str], text: str) -> bool:
    return any(re.search(re.escape(w), text, re.IGNORECASE) for w in words)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. æ²ç¤ºæ¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_threads():
    threads, seen_ids = [], set()
    headers = {"User-Agent": "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"}
    for page in range(1, MAX_PAGES + 1):
        url = f"https://www.e-mansion.co.jp/bbs/board/23ku/?page={page}"
        res = requests.get(url, headers=headers, timeout=30)
        if res.status_code != 200:
            continue
        blocks = re.findall(
            r'<a href="/bbs/thread/(\d+)/" class="component_thread_list_item link.*?<span class="num_of_item">(\d+)</span>.*?<div class="oneliner title"[^>]*>(.*?)</div>',
            res.text, re.DOTALL)
        for tid, cnt, ttl in blocks:
            if tid in seen_ids:             # â˜… é‡è¤‡IDã‚’é™¤å¤–
                continue
            seen_ids.add(tid)
            threads.append({
                "url": f"https://www.e-mansion.co.jp/bbs/thread/{tid}/",
                "id": tid,
                "title": html.unescape(ttl).strip(),
                "count": int(cnt)
            })
    return threads

def fetch_thread_posts(tid: str, max_pages: int = 3, delay=0.3) -> list[str]:
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    posts = []
    for p in range(1, max_pages + 1):
        url = f"https://www.e-mansion.co.jp/bbs/thread/{tid}/?page={p}"
        try:
            r = requests.get(url, headers=headers, timeout=15)
            r.raise_for_status()
        except requests.RequestException:
            continue
        soup = BeautifulSoup(r.text, "html.parser")
        posts += [t.get_text(strip=True) for t in soup.select('p[itemprop="commentText"]')]
        time.sleep(delay)
    return posts

def fetch_thread_text(url: str) -> str:
    tid = re.search(r'/thread/(\d+)/', url).group(1)
    return "\n".join(fetch_thread_posts(tid))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ I/O
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_history() -> dict[str, int]:
    sheet = gc.open_by_key(SPREADSHEET_ID).worksheet(HISTORY_SHEET)
    return {r[0]: int(r[1]) for r in sheet.get_all_values()[1:]
            if len(r) > 1 and r[1].isdigit()}

def save_history(hist: dict[str, int]):
    ws = gc.open_by_key(SPREADSHEET_ID).worksheet(HISTORY_SHEET)
    ws.clear()
    ws.append_row(["URL", "å–å¾—æ™‚ãƒ¬ã‚¹æ•°", "æœ€çµ‚å–å¾—æ—¥"])
    ws.append_rows([[u, c, datetime.date.today().isoformat()] for u, c in hist.items()])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. Claude API ãƒ©ãƒƒãƒ‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def claude_call(prompt: str, max_tokens: int):
    headers = {"x-api-key": CLAUDE_API_KEY, "anthropic-version": "2023-06-01"}
    body = {
        "model": "claude-3-haiku-20240307",
        "temperature": 0,
        "max_tokens": max_tokens,
        "messages": [{"role": "user", "content": prompt}]
    }
    res = requests.post("https://api.anthropic.com/v1/messages", headers=headers,
                        json=body, timeout=45)
    res.raise_for_status()
    return res.json()["content"][0]["text"].strip()

def judge_risk(text: str):
    prompt = f"""SNS ç‚ä¸Šãƒªã‚¹ã‚¯ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ã—ã¦ãã ã•ã„ã€‚
æœ¬æ–‡ï¼ˆæ—¥æœ¬èªï¼‰ã«ã¤ã„ã¦ã€ç‚ä¸Šã«ã¤ãªãŒã‚‹è¦ç´ ãŒã‚ã‚‹ã‹å³æ ¼ã«åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

## åˆ¤å®šåŸºæº–
â–¼ ãƒªã‚¹ã‚¯ï¼šé«˜
1. äººç¨®ãƒ»å›½ç±ãƒ»å®—æ•™ãƒ»æ€§åˆ¥ãƒ»åœ°åŸŸãªã©ã® **å±æ€§å·®åˆ¥**ã€èª¹è¬—ä¸­å‚·ã€è”‘ç§°ã€ãƒ˜ã‚¤ãƒˆè¡¨ç¾
2. å…¬åºè‰¯ä¿—ã«åã™ã‚‹ **é•æ³•è¡Œç‚ºã®åŠ©é•·**ãƒ»æ¨å¥¨ãƒ»è‡ªæ…¢
3. ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–ãªäº‹ä»¶ãƒ»ç½å®³ãƒ»æ”¿æ²»ãƒ»å®—æ•™ã«é–¢ã™ã‚‹ **ä¸€æ–¹çš„ã¾ãŸã¯æ‰‡å‹•çš„è¡¨ç¾**
4. å€‹äººã‚„ä¼æ¥­ã¸ã® **æ”»æ’ƒçš„ãƒ»æŒ‘ç™ºçš„ãªè¨€åŠ**ã€åèª‰æ¯€æã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼æš´éœ²
5. è™å¾…ãƒ»æ®‹è™ãƒ»æ€§çš„æ¾å–ãªã© **ä¸å¿«ãƒ»æš´åŠ›çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„**
â–¼ ãƒªã‚¹ã‚¯ï¼šä½
ä¸Šè¨˜ 1â€“5 ã®ã„ãšã‚Œã«ã‚‚ **è©²å½“ã—ãªã„** å ´åˆã€‚
## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆå³å®ˆï¼‰
- 1 è¡Œç›®ï¼š**ã€Œãƒªã‚¹ã‚¯ï¼šé«˜ã€** ã¾ãŸã¯ **ã€Œãƒªã‚¹ã‚¯ï¼šä½ã€** ã®ã¿
- 2 è¡Œç›®ï¼šåˆ¤å®šç†ç”±ï¼ˆæ¡ä»¶ç•ªå·ã‚’æ˜è¨˜ã™ã‚‹ã¨ãƒ™ã‚¿ãƒ¼ï¼‰
- 3 è¡Œç›®ä»¥é™ã¯ä½•ã‚‚æ›¸ã‹ãªã„
- æ¡ä»¶ã‚’æº€ãŸã›ãªã„å ´åˆã¯ **ã€ŒERRORã€** ã¨ã ã‘æ›¸ã
--- æœ¬æ–‡ ---
{text}"""
    try:
        msg = claude_call(prompt, 200)
        risk = "é«˜" if "ãƒªã‚¹ã‚¯ï¼šé«˜" in msg else "ä½"
        return risk, msg, ("NG" if risk == "é«˜" else "OK")
    except Exception as e:
        return "é«˜", f"[Error] {e}", "NG"

def generate_summary(text: str, max_retry=MAX_RETRY_BASE):
    base_prompt = f"""ã‚ãªãŸã¯ Xï¼ˆæ—§Twitterï¼‰å‘ã‘ã®ã‚³ãƒ”ãƒ¼ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
æ²ç¤ºæ¿ã‚¹ãƒ¬ãƒƒãƒ‰æœ¬æ–‡ã‚’èª­ã¿ã€èª­è€…ãŒç¶šãã‚’ã‚¯ãƒªãƒƒã‚¯ã—ãŸããªã‚‹ **å‰å‘ãã§é•·ã‚** ã®æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ã‚’ 1 æœ¬ã ã‘ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
### å‡ºåŠ›ä»•æ§˜ï¼ˆå¿…ãšå®ˆã‚‹ï¼‰
1. **ã‚¿ã‚¤ãƒˆãƒ«æœ¬æ–‡ã®ã¿** ã‚’ 1 è¡Œã§å‡ºåŠ›
   - è¡Œé ­ã«ã€Œ-ã€ã€Œâ€“ã€ã€Œãƒ»ã€ãªã©è¨˜å·ã‚’ä»˜ã‘ãªã„
   - æ¥é ­è¾ã€Œã‚¿ã‚¤ãƒˆãƒ«:ã€ã‚„è§£èª¬æ–‡ï¼ˆä¾‹: ã€Œç¦æ­¢èªã‚’å«ã¾ãšï½ã€ã€Œ90æ–‡å­—ä»¥å†…ã§ï½ã€ï¼‰ã‚’ä»˜ã‘ãªã„
   - åŒä¸€ã®æ–‡ã‚’ 2 å›ä»¥ä¸Šç¹°ã‚Šè¿”ã•ãªã„
   - æ”¹è¡Œãƒ»ã‹ãæ‹¬å¼§ãƒ»ç®‡æ¡æ›¸ããƒ»çµµæ–‡å­—ãƒ»è¨˜å·èª¬æ˜ã‚’ä»˜ã‘ãªã„
   - 90 æ–‡å­—ä»¥å†…
2. æœ¬æ–‡å†’é ­ã« **åœ°åãƒ»é§…åãƒ»æ•°å­—** ã„ãšã‚Œã‹ã‚’å…¥ã‚Œã¦ç›®ã‚’å¼•ãæ§‹æˆã«ã™ã‚‹
3. æœ«å°¾ã« **4ã‹ã‚‰7æ–‡å­—ç¨‹åº¦ã® CTA** ã‚’ä»˜ã‘ã‚‹ï¼ˆä¾‹: è©³ã—ãã¯ã“ã¡ã‚‰ğŸ‘‡ã€ç¶šãã¯ã“ã¡ã‚‰ğŸ‘‡ï¼‰
4. ä¸Šè¨˜ã‚’ 1 ã¤ã§ã‚‚æº€ãŸã›ãªã„ã€ã¾ãŸã¯ç¦æ­¢èªã‚’ 1 èªã§ã‚‚å«ã‚€å ´åˆã¯ **NOK** ã¨ã ã‘å‡ºåŠ›ã™ã‚‹
   - NOK ä»¥å¤–ã®æ–‡å­—ã‚’ä¸€åˆ‡æ›¸ã‹ãªã„
### ç¦æ­¢èª
{', '.join(BANNED_WORDS)}
--- æœ¬æ–‡ ---
{text}"""
    for _ in range(max_retry + 1):
        try:
            title = claude_call(base_prompt, 80)
        except Exception:
            time.sleep(2); continue

        if "NOK" in title.upper():
            return "NOK"

        title = re.sub(r'\s+', ' ', title.strip())
        title = re.sub(r'^\s*ã‚¿ã‚¤ãƒˆãƒ«[:ï¼š]\s*', '', title)
        title = re.sub(r'^.*?[ã€Œ"](.*?)[ã€"]$', r'\1', title)

        if contains_banned(BANNED_WORDS, title):
            time.sleep(1); continue
        if len(title) > MAX_TITLE_LEN:
            title = title[:MAX_TITLE_LEN].rstrip("ã€,ã€‚. ") + "â€¦"
        return title + CTA
    return "NOK"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. ãƒ¡ã‚¤ãƒ³
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print("â–¶ main() start")
    threads  = fetch_threads()
    history  = load_history()
    diffs    = []

    # ä¸Šä½ 20 ã‚¹ãƒ¬æŠ½å‡º
    for t in sorted(threads,
                    key=lambda x: x["count"] - history.get(x["url"], 0),
                    reverse=True):
        if t["url"] in history and t["count"] - history[t["url"]] <= 0:
            continue
        if t["url"] not in history and t["count"] < 100:
            continue
        diffs.append(t)
        if len(diffs) == 20:
            break

    # ç‚ä¸Šãƒªã‚¹ã‚¯åˆ¤å®š
    candidates, updated = [], {}
    for d in diffs:
        txt = fetch_thread_text(d["url"])
        risk, msg, flag = judge_risk(txt)
        candidates.append({**d, "risk": risk, "comment": msg, "flag": flag})
        updated[d["url"]] = d["count"]

    ok = [c for c in candidates if c["flag"] == "OK"][:POST_COUNT]
    random.shuffle(ok)

    # æŠ•ç¨¿äºˆå®šã‚·ãƒ¼ãƒˆæº–å‚™
    ws_post = gc.open_by_key(SPREADSHEET_ID).worksheet(POST_SHEET)
    ws_post.clear()
    ws_post.append_row(["æ—¥ä»˜", "æŠ•ç¨¿æ™‚é–“", "æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆ", "æŠ•ç¨¿æ¸ˆã¿", "URL"])

    today = datetime.date.today()
    base_monday = today + datetime.timedelta(days=((7 - today.weekday()) % 7 or 7))
    scheduled = set()                 # â˜… é‡è¤‡URLé˜²æ­¢

    for idx, c in enumerate(ok):
        if c["url"] in scheduled:
            continue
        scheduled.add(c["url"])

        title = "NOK"
        for _ in range(MAX_EXTRA_RETRY + 1):
            title = generate_summary(fetch_thread_text(c["url"]))
            if title.upper() != "NOK":
                break
        if title.upper() == "NOK":
            continue

        post_date = base_monday + datetime.timedelta(days=idx // 2)
        time_str  = "8:00" if idx % 2 == 0 else "15:00"
        tid       = re.search(r'/thread/(\d+)/', c["url"]).group(1)
        utm       = f"?utm_source=x&utm_medium=em-{tid}&utm_campaign={post_date:%Y%m%d}"
        post_txt  = f"{title}\n#ãƒãƒ³ã‚·ãƒ§ãƒ³ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£\n{c['url']}{utm}"
        ws_post.append_row([post_date.strftime("%Y/%m/%d"), time_str,
                            post_txt, "FALSE", c["url"]])

    # å±¥æ­´ä¿å­˜ï¼ˆTEST_MODE=1 ã®ã¨ãã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    if os.getenv("TEST_MODE") != "1":
        save_history({**history, **{u: updated[u] for u in scheduled}})

    print("â–¶ Done")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    main()
